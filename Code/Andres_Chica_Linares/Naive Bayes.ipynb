{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e6bac3-b090-4d49-b92a-5fccd6015060",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bibliotecas e información de la sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe4ff07-8c95-44b7-b5b4-ced4cb4cd079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-- \u001b[1mAttaching packages\u001b[22m ------------------------------------------------------------------------------- tidyverse 1.3.1 --\n",
      "\n",
      "\u001b[32mv\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.5     \u001b[32mv\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32mv\u001b[39m \u001b[34mtibble \u001b[39m 3.1.5     \u001b[32mv\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.7\n",
      "\u001b[32mv\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.4     \u001b[32mv\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32mv\u001b[39m \u001b[34mreadr  \u001b[39m 2.0.2     \u001b[32mv\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
      "\n",
      "-- \u001b[1mConflicts\u001b[22m ---------------------------------------------------------------------------------- tidyverse_conflicts() --\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: 'caret'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:purrr':\n",
      "\n",
      "    lift\n",
      "\n",
      "\n",
      "AUC 0.3.0\n",
      "\n",
      "Type AUCNews() to see the change log and ?AUC to get an overview.\n",
      "\n",
      "\n",
      "Attaching package: 'AUC'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:caret':\n",
      "\n",
      "    sensitivity, specificity\n",
      "\n",
      "\n",
      "Loading required package: colorspace\n",
      "\n",
      "Loading required package: grid\n",
      "\n",
      "VIM is ready to use.\n",
      "\n",
      "\n",
      "Suggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n",
      "\n",
      "\n",
      "Attaching package: 'VIM'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:datasets':\n",
      "\n",
      "    sleep\n",
      "\n",
      "\n",
      "Loading required package: randomForest\n",
      "\n",
      "randomForest 4.6-14\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "\n",
      "Loading required package: foreach\n",
      "\n",
      "\n",
      "Attaching package: 'foreach'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:purrr':\n",
      "\n",
      "    accumulate, when\n",
      "\n",
      "\n",
      "Loading required package: itertools\n",
      "\n",
      "Loading required package: iterators\n",
      "\n",
      "\n",
      "Attaching package: 'missForest'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:VIM':\n",
      "\n",
      "    nrmse\n",
      "\n",
      "\n",
      "Warning message:\n",
      "\"package 'mice' was built under R version 4.1.2\"\n",
      "\n",
      "Attaching package: 'mice'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    filter\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    cbind, rbind\n",
      "\n",
      "\n",
      "Loading required package: MASS\n",
      "\n",
      "\n",
      "Attaching package: 'MASS'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    select\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las librerías\n",
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(caret)\n",
    "library(RWeka)\n",
    "library(AUC)\n",
    "library(VIM)\n",
    "library(missForest)\n",
    "library(mice)\n",
    "library(klaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80ab655-47e3-4e99-bbcf-412fc8947806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R version 4.1.1 (2021-08-10)\n",
       "Platform: x86_64-w64-mingw32/x64 (64-bit)\n",
       "Running under: Windows 10 x64 (build 19043)\n",
       "\n",
       "Matrix products: default\n",
       "\n",
       "locale:\n",
       "[1] LC_COLLATE=Spanish_Spain.1252  LC_CTYPE=Spanish_Spain.1252   \n",
       "[3] LC_MONETARY=Spanish_Spain.1252 LC_NUMERIC=C                  \n",
       "[5] LC_TIME=Spanish_Spain.1252    \n",
       "\n",
       "attached base packages:\n",
       "[1] grid      stats     graphics  grDevices utils     datasets  methods  \n",
       "[8] base     \n",
       "\n",
       "other attached packages:\n",
       " [1] klaR_0.6-15         MASS_7.3-54         mice_3.13.0        \n",
       " [4] missForest_1.4      itertools_0.1-3     iterators_1.0.13   \n",
       " [7] foreach_1.5.1       randomForest_4.6-14 VIM_6.1.1          \n",
       "[10] colorspace_2.0-2    AUC_0.3.0           RWeka_0.4-43       \n",
       "[13] caret_6.0-90        lattice_0.20-45     forcats_0.5.1      \n",
       "[16] stringr_1.4.0       dplyr_1.0.7         purrr_0.3.4        \n",
       "[19] readr_2.0.2         tidyr_1.1.4         tibble_3.1.5       \n",
       "[22] ggplot2_3.3.5       tidyverse_1.3.1    \n",
       "\n",
       "loaded via a namespace (and not attached):\n",
       "  [1] readxl_1.3.1         uuid_1.0-2           backports_1.3.0     \n",
       "  [4] plyr_1.8.6           repr_1.1.3           sp_1.4-5            \n",
       "  [7] splines_4.1.1        listenv_0.8.0        digest_0.6.28       \n",
       " [10] htmltools_0.5.2      fansi_0.5.0          magrittr_2.0.1      \n",
       " [13] tzdb_0.2.0           openxlsx_4.2.4       recipes_0.1.17      \n",
       " [16] globals_0.14.0       modelr_0.1.8         gower_0.2.2         \n",
       " [19] rvest_1.0.2          haven_2.4.3          crayon_1.4.2        \n",
       " [22] jsonlite_1.7.2       survival_3.2-13      zoo_1.8-9           \n",
       " [25] glue_1.4.2           gtable_0.3.0         ipred_0.9-12        \n",
       " [28] questionr_0.7.5      car_3.0-11           future.apply_1.8.1  \n",
       " [31] DEoptimR_1.0-9       abind_1.4-5          scales_1.1.1        \n",
       " [34] DBI_1.1.1            miniUI_0.1.1.1       Rcpp_1.0.7          \n",
       " [37] xtable_1.8-4         laeken_0.5.2         foreign_0.8-81      \n",
       " [40] proxy_0.4-26         stats4_4.1.1         lava_1.6.10         \n",
       " [43] prodlim_2019.11.13   vcd_1.4-9            httr_1.4.2          \n",
       " [46] ellipsis_0.3.2       pkgconfig_2.0.3      rJava_1.0-5         \n",
       " [49] nnet_7.3-16          dbplyr_2.1.1         utf8_1.2.2          \n",
       " [52] tidyselect_1.1.1     rlang_0.4.12         reshape2_1.4.4      \n",
       " [55] later_1.3.0          munsell_0.5.0        cellranger_1.1.0    \n",
       " [58] tools_4.1.1          cli_3.1.0            generics_0.1.1      \n",
       " [61] ranger_0.13.1        broom_0.7.10         evaluate_0.14       \n",
       " [64] fastmap_1.1.0        ModelMetrics_1.2.2.2 fs_1.5.0            \n",
       " [67] zip_2.2.0            robustbase_0.93-9    future_1.23.0       \n",
       " [70] nlme_3.1-153         mime_0.12            xml2_1.3.2          \n",
       " [73] compiler_4.1.1       rstudioapi_0.13      curl_4.3.2          \n",
       " [76] e1071_1.7-9          reprex_2.0.1         stringi_1.7.5       \n",
       " [79] highr_0.9            IRdisplay_1.0        Matrix_1.3-4        \n",
       " [82] RWekajars_3.9.3-2    vctrs_0.3.8          pillar_1.6.4        \n",
       " [85] lifecycle_1.0.1      combinat_0.0-8       lmtest_0.9-39       \n",
       " [88] data.table_1.14.2    httpuv_1.6.5         R6_2.5.1            \n",
       " [91] promises_1.2.0.1     rio_0.5.27           parallelly_1.28.1   \n",
       " [94] codetools_0.2-18     boot_1.3-28          assertthat_0.2.1    \n",
       " [97] withr_2.4.2          parallel_4.1.1       hms_1.1.1           \n",
       "[100] labelled_2.9.0       rpart_4.1-15         timeDate_3043.102   \n",
       "[103] IRkernel_1.2         class_7.3-19         carData_3.0-4       \n",
       "[106] pbdZMQ_0.3-6         pROC_1.18.0          shiny_1.7.1         \n",
       "[109] lubridate_1.8.0      base64enc_0.1-3     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se informa de las características y el entorno en el que se ha desarrolado el trabajo\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff1716-16b8-45de-9868-e5296d620984",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb410f2-532a-4bf4-8931-5899af84c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos de entrenamiento\n",
    "# Imputación por MICE\n",
    "tra_feat_noNAmice = read.csv('X_train_noNAmice.csv', header=TRUE, sep=',',\n",
    "                    na.strings = c('?','', 'NA'))\n",
    "# Imputación por knn\n",
    "tra_feat_noNAknn = read.csv('train_features_knn_imput.csv', header=TRUE, sep=',',\n",
    "                    na.strings = c('?','', 'NA'))\n",
    "# Imputación por miss Forest\n",
    "tra_feat_noNAmissF = read.csv('train_features_missF_imput.csv', header=TRUE, sep=',',\n",
    "                    na.strings = c('?','', 'NA'))\n",
    "# Datos originales\n",
    "tra_feat_original = read.csv('training_set_features.csv', header=TRUE, sep=',',\n",
    "                    na.strings = c('?','', 'NA'))\n",
    "# Cargar labels de entrenamiento\n",
    "tra_lab = read.csv('training_set_labels.csv', header=TRUE, sep=',',\n",
    "                    na.strings = c('?','', 'NA'))\n",
    "# Cargar datos de entrenamiento\n",
    "# Imputación por MICE\n",
    "test_feat_noNAmice = read.csv('X_test_noNAmice.csv', header=TRUE, sep=',',\n",
    "                    na.strings = c('?','', 'NA'))\n",
    "# Imputación por knn\n",
    "test_feat_noNAknn = read.csv('test_features_knn_imput.csv', header=TRUE, sep=',',\n",
    "                    na.strings = c('?','', 'NA'))\n",
    "# Imputación por miss Forest\n",
    "test_feat_noNAmissF = read.csv('test_features_missF_imput.csv', header=TRUE, sep=',',\n",
    "                    na.strings = c('?','', 'NA'))\n",
    "# Datos originales\n",
    "test_feat_original = read.csv('test_set_features.csv', header=TRUE, sep=',',\n",
    "                    na.strings = c('?','', 'NA'))\n",
    "# Cargar el modelo para las entregas\n",
    "submission = read.csv('submission_format.csv', header=TRUE, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4607c0-2d06-46be-990e-398e22e28b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos las variables de los dataframe a factor\n",
    "tra_feat_noNAmice = as.data.frame((lapply(tra_feat_noNAmice[, -1], as.factor)))\n",
    "tra_feat_noNAknn = as.data.frame((lapply(tra_feat_noNAknn[, -1], as.factor)))\n",
    "tra_feat_noNAmissF = as.data.frame((lapply(tra_feat_noNAmissF[, -1], as.factor)))\n",
    "tra_feat_original = as.data.frame((lapply(tra_feat_original[, -1], as.factor)))\n",
    "tra_lab = as.data.frame(lapply(tra_lab[, -1], as.factor))\n",
    "test_feat_noNAmice = as.data.frame((lapply(test_feat_noNAmice[, -1], as.factor)))\n",
    "test_feat_noNAknn = as.data.frame((lapply(test_feat_noNAknn[, -1], as.factor)))\n",
    "test_feat_noNAmissF = as.data.frame((lapply(test_feat_noNAmissF[, -1], as.factor)))\n",
    "test_feat_original = as.data.frame((lapply(test_feat_original[, -1], as.factor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6facbae2-0838-44d5-97a1-57c09b449f1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d37db-33c6-49f4-844e-e9d41b10d052",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d899918-8b20-4d98-99dd-b20e312765fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Forest CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88e056-cdb4-4b0e-bde3-30ae595f4b1d",
   "metadata": {},
   "source": [
    "Se crean divisiones del data set de entrenamiento que se usarán para entrenar y evaluar el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305690a2-f0a9-4a92-aa41-2b447763be3b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Se crean subsets de los datos para entrenamiento y evaluación\n",
    "shuffle_ds = sample(dim(tra_feat_noNAmice)[1])\n",
    "pct90 = (dim(tra_feat_noNAmice)[1] * 90) %/% 100\n",
    "tra_feat_sam = tra_feat_noNAmice[shuffle_ds[1:pct90], ]\n",
    "tra_lab_sam = tra_lab[shuffle_ds[1:pct90], ]\n",
    "tst_feat_sam = tra_feat_noNAmice[shuffle_ds[(pct90+1):dim(tra_feat_noNAmice)[1]], ]\n",
    "tst_lab_sam = tra_lab[shuffle_ds[(pct90+1):dim(tra_lab)[1]], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc1c93ec-0e61-48fd-8589-f8a784d37b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una función que se usa para testear el rendimiento del forest ensemble\n",
    "accuracy_of_ensemble_h1n1 <- function(method, metric, tst_feat_sam, tra_feat_sam, ctrl, grid, b = 100) {\n",
    "    # Se crea la variable en la que se guardarán los datos\n",
    "    Pred_h1n1_prob = data.frame(X0 = rep(0.0, dim(tst_feat_sam)[1]), X1 = rep(0.0, dim(tst_feat_sam)[1]))\n",
    "    # Se calcula el 5 porciento del número de muestras\n",
    "    pct05 = (dim(tra_feat_sam)[1] * 5) %/% 100\n",
    "    # Para cada división del dataset\n",
    "    for (i in 1:b){\n",
    "        # Se toma una muestra aleatoria\n",
    "        shuffle_ds = sample(dim(tra_feat_sam)[1])\n",
    "        ind_feat = sample(length(colnames(tra_feat_sam)), 6)\n",
    "        # Se entrena un clasificador con esta muestra\n",
    "        Fit_h1n1_i = train(tra_feat_sam[shuffle_ds[1:pct05], ind_feat], make.names(tra_lab_sam[shuffle_ds[1:pct05], 1]),\n",
    "                      method = method,\n",
    "                      metric = \"ROC\",\n",
    "                      trControl = ctrl,\n",
    "                      tuneGrid = grid)\n",
    "        # Se predice con el clasificador la probabilidad de las muestras de test correspondiente a esta división\n",
    "        Pred_h1n1_prob_i = predict(Fit_h1n1_i, newdata = tst_feat_sam, type = \"prob\")\n",
    "        # Se añade la probabilidad ponderada\n",
    "        Pred_h1n1_prob = Pred_h1n1_prob + (Pred_h1n1_prob_i / b)\n",
    "    }\n",
    "    # Se muestra y devuelve el rendimiento\n",
    "    cat(\"AUC:\", auc(roc(Pred_h1n1_prob$X1, tst_lab_sam$h1n1_vaccine)))\n",
    "    return(auc(roc(Pred_h1n1_prob$X1, tst_lab_sam$h1n1_vaccine))) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea403dd7-2763-4dfe-b887-d64f2428cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el control del entrenamiento y los parámetros del clasificador\n",
    "ctrl <- trainControl(method=\"cv\", number=5, classProbs= TRUE, summaryFunction = twoClassSummary)\n",
    "grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE),.adjust=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6094b86d-c925-4a2a-90f4-bd6367f2a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7882392"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.788239195877343"
      ],
      "text/latex": [
       "0.788239195877343"
      ],
      "text/markdown": [
       "0.788239195877343"
      ],
      "text/plain": [
       "[1] 0.7882392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(warn = -1)\n",
    "# Se comprueba el rendimiento del clasificador en los datos de entrenamiento para comprobar el funcionamiento correcto\n",
    "accuracy_of_ensemble_h1n1(method = \"nb\", metric = \"ROC\", tst_feat_sam, tra_feat_sam, ctrl, grid)\n",
    "options(warn = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a918979-60f0-424b-943c-7027e32566d7",
   "metadata": {},
   "source": [
    "#### Forest test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbbc4344-d553-4089-a36c-0a8eb6d63c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una función que se usa para calcular la probabilidad de la salida h1n1\n",
    "predict_with_ensemble_h1n1 <- function(method, metric, test_features, tra_feat, tra_lab, ctrl, grid, b = 100) {\n",
    "    # Se crea la variable en la que se guardarán los datos\n",
    "    Pred_h1n1_prob = data.frame(X0 = rep(0.0, dim(test_features)[1]), X1 = rep(0.0, dim(test_features)[1]))\n",
    "    # Se calcula el 5 porciento del número de muestras\n",
    "    pct05 = (dim(tra_feat)[1] * 5) %/% 100\n",
    "    # Para cada división del dataset\n",
    "    for (i in 1:b){\n",
    "        # Se toma una muestra aleatoria\n",
    "        shuffle_ds = sample(dim(tra_feat)[1])\n",
    "        ind_feat = sample(length(colnames(tra_feat)), 6)\n",
    "        # Se entrena un clasificador con esta muestra\n",
    "        Fit_h1n1_i = train(tra_feat[shuffle_ds[1:pct05], ind_feat], make.names(tra_lab[shuffle_ds[1:pct05], 1]),\n",
    "                        method = method,\n",
    "                        metric = metric,\n",
    "                        trControl = ctrl)\n",
    "\n",
    "        # Se predice con el clasificador la probabilidad de las muestras de test correspondiente a esta división\n",
    "        Pred_h1n1_prob_i = predict(Fit_h1n1_i, newdata = test_features, type = \"prob\")\n",
    "        # Se añade la probabilidad ponderada\n",
    "        Pred_h1n1_prob = Pred_h1n1_prob + (Pred_h1n1_prob_i / b)\n",
    "        }\n",
    "    # Se devuelve la probabilidad calculada\n",
    "    return(Pred_h1n1_prob$X1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d468b8bf-246d-4b63-b659-90fa7e635fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una función que se usa para calcular la probabilidad de la salida seasonal\n",
    "predict_with_ensemble_seas <- function(method, metric, test_features, tra_feat, tra_lab, ctrl, grid, b = 100) {\n",
    "    # Se crea la variable en la que se guardarán los datos\n",
    "    Pred_seas_prob = data.frame(X0 = rep(0.0, dim(test_features)[1]), X1 = rep(0.0, dim(test_features)[1]))\n",
    "    # Se calcula el 5 porciento del número de muestras\n",
    "    pct05 = (dim(tra_feat)[1] * 5) %/% 100\n",
    "    # Para cada división del dataset\n",
    "    for (i in 1:b){\n",
    "        # Se toma una muestra aleatoria\n",
    "        shuffle_ds = sample(dim(tra_feat)[1])\n",
    "        ind_feat = sample(length(colnames(tra_feat)), 6)        \n",
    "        \n",
    "        # Se entrena un clasificador con esta muestra\n",
    "        Fit_seas_i = train(tra_feat[shuffle_ds[1:pct05], ind_feat], make.names(tra_lab[shuffle_ds[1:pct05], 2]),\n",
    "                  method = method,\n",
    "                  metric = metric,\n",
    "                  trControl = ctrl)\n",
    "  \n",
    "        # Se predice con el clasificador la probabilidad de las muestras de test correspondiente a esta división\n",
    "        Pred_seas_prob_i = predict(Fit_seas_i, newdata = test_features, type = \"prob\")\n",
    "        # Se añade la probabilidad ponderada\n",
    "        Pred_seas_prob = Pred_seas_prob + (Pred_seas_prob_i / b)\n",
    "        }\n",
    "    # Se devuelve la probabilidad calculada\n",
    "    return(Pred_seas_prob$X1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7aa3c83-dd38-452d-ba89-b31ad4138c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn = -1)\n",
    "# Se calcula la probabilidad dada por el algoritmo con los datos imputados con mice para ambas salidas\n",
    "submission$h1n1_vaccine = predict_with_ensemble_h1n1(\"nb\", \"ROC\", test_feat_noNAmice, tra_feat_noNAmice, tra_lab, ctrl, grid)\n",
    "submission$seasonal_vaccine = predict_with_ensemble_seas(\"nb\", \"ROC\", test_feat_noNAmice, tra_feat_noNAmice, tra_lab, ctrl, grid)\n",
    "options(warn = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6219c19-16b0-4ac8-9f47-327f67b49f7c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Se guarda el resultado para subirlo a la plataforma\n",
    "write_csv(submission, \"submission_nb_mice_100forest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "746eb7bd-b2d1-411d-bf16-8aa5feccd644",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn = -1)\n",
    "# Se calcula la probabilidad dada por el algoritmo con los datos imputados con miss forest para ambas salidas\n",
    "submission$h1n1_vaccine = predict_with_ensemble_h1n1(\"nb\", \"ROC\", test_feat_noNAmissF, tra_feat_noNAmissF, tra_lab, ctrl, grid)\n",
    "submission$seasonal_vaccine = predict_with_ensemble_seas(\"nb\", \"ROC\", test_feat_noNAmissF, tra_feat_noNAmissF, tra_lab, ctrl, grid)\n",
    "options(warn = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0496b818-6ee9-4144-94dc-887add8a4978",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Se guarda el resultado para subirlo a la plataforma\n",
    "write_csv(submission, \"submission_nb_missF_100forest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5094eca2-6cf3-4aab-b60e-c4075304d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn = -1)\n",
    "# Se calcula la probabilidad dada por el algoritmo con los datos imputados con knn para ambas salidas\n",
    "submission$h1n1_vaccine = predict_with_ensemble_h1n1(\"nb\", \"ROC\", test_feat_noNAknn, tra_feat_noNAknn, tra_lab, ctrl, grid)\n",
    "submission$seasonal_vaccine = predict_with_ensemble_seas(\"nb\", \"ROC\", test_feat_noNAknn, tra_feat_noNAknn, tra_lab, ctrl, grid)\n",
    "options(warn = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c52a820-cad8-4f07-b1e3-c9d03908e329",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Se guarda el resultado para subirlo a la plataforma\n",
    "write_csv(submission, \"submission_nb_knn_100forest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af864b6c-0c57-4fa3-8b74-4a7819ae3dd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ensemble OVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82189d7-a9f4-4806-84c2-085e97d771ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Función OVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7007d392-6c48-468c-9340-5a7c1eea2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una función que se usa para testear el rendimiento del forest ensemble\n",
    "predict_with_ova = function(submission, method, metric, tst_feat, tra_feat, tra_lab, ctrl, grid){\n",
    "    # Se crea una columna de clase en la que se codifican ambas salidas\n",
    "    tra_lab = tra_lab %>% mutate(clase = 2 * as.numeric(as.character(h1n1_vaccine)) + as.numeric(as.character(seasonal_vaccine)))\n",
    "    # Se crea una columna de clasex para cada valor de la combinación de salidas que se pueda dar\n",
    "    tra_lab = tra_lab %>% mutate(clase0 = ifelse(clase == 0, 1, 0),\n",
    "                                 clase1 = ifelse(clase == 1, 1, 0),\n",
    "                                 clase2 = ifelse(clase == 2, 1, 0),\n",
    "                                 clase3 = ifelse(clase == 3, 1, 0))\n",
    "    # Se redimensiona y se convierte a factor\n",
    "    tra_lab = as.data.frame(lapply(tra_lab[, -1], as.factor))\n",
    "\n",
    "    # Se entrena un clasificador para el caso en el que h1n1 = 0 y seasonal = 0\n",
    "    Fit_c0 = train(tra_feat, make.names(tra_lab$clase0),\n",
    "                    method = method,\n",
    "                    metric = metric,\n",
    "                    trControl = ctrl)\n",
    "    # Se predice la probabilidad de que la salida multiclase sea el caso correspondiente\n",
    "    Pred_c0_prob = predict(Fit_c0, newdata = tst_feat, type = \"prob\")\n",
    "\n",
    "    # Se entrena un clasificador para el caso en el que h1n1 = 0 y seasonal = 1\n",
    "    Fit_c1 = train(tra_feat, make.names(tra_lab$clase1),\n",
    "                    method = method,\n",
    "                    metric = metric,\n",
    "                    trControl = ctrl)\n",
    "    # Se predice la probabilidad de que la salida multiclase sea el caso correspondiente\n",
    "    Pred_c1_prob = predict(Fit_c1, newdata = tst_feat, type = \"prob\")\n",
    "\n",
    "    # Se entrena un clasificador para el caso en el que h1n1 = 1 y seasonal = 0\n",
    "    Fit_c2 = train(tra_feat, make.names(tra_lab$clase2),\n",
    "                    method = method,\n",
    "                    metric = metric,\n",
    "                    trControl = ctrl)\n",
    "    # Se predice la probabilidad de que la salida multiclase sea el caso correspondiente\n",
    "    Pred_c2_prob = predict(Fit_c2, newdata = tst_feat, type = \"prob\")\n",
    "\n",
    "    # Se entrena un clasificador para el caso en el que h1n1 = 1 y seasonal = 1\n",
    "    Fit_c3 = train(tra_feat, make.names(tra_lab$clase3),\n",
    "                    method = method,\n",
    "                    metric = metric,\n",
    "                    trControl = ctrl)\n",
    "    # Se predice la probabilidad de que la salida multiclase sea el caso correspondiente\n",
    "    Pred_c3_prob = predict(Fit_c3, newdata = tst_feat, type = \"prob\")\n",
    "    \n",
    "    # Se hace una ponderación de las probabilidades de cada clase de las 4 multiclases para hallar la probabilidad de cada salida por separado\n",
    "    submission$h1n1_vaccine = (((Pred_c2_prob$X1 + Pred_c3_prob$X1)/2 + (1 - (Pred_c0_prob$X1 + Pred_c1_prob$X1)/2))/2)\n",
    "    submission$seasonal_vaccine = (((Pred_c1_prob$X1 + Pred_c3_prob$X1)/2+ (1 - (Pred_c0_prob$X1 + Pred_c2_prob$X1)/2))/2)\n",
    "    \n",
    "    # Se devuelve el dataset para enviarlo a la plataforma\n",
    "    return(submission)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c868da-8e67-46d0-8aa5-f502c5cac5c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Validación con los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c59366f-0d2d-4e8d-bb45-b2ac8c1a38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el control del entrenamiento y los parámetros del clasificador\n",
    "ctrl <- trainControl(method=\"cv\", number=5, classProbs= TRUE, summaryFunction = twoClassSummary)\n",
    "grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE),.adjust=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10743a8a-d950-4a0d-851c-ef47cc09e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean subsets de los datos de entrenamiento para entrenamiento y evaluación\n",
    "submission_nb_mice_ova_sub = submission[as.integer(dim(tra_feat_noNAmice)[1]*0.8):dim(tra_feat_noNAmice)[1],]\n",
    "tra_feat_noNAmice_sub = tra_feat_noNAmice[1:as.integer(dim(tra_feat_noNAmice)[1]*0.8),]\n",
    "tra_lab_sub = tra_lab[1:as.integer(dim(tra_feat_noNAmice)[1]*0.8),]\n",
    "test_feat_noNAmice_sub = tra_feat_noNAmice[as.integer(dim(tra_feat_noNAmice)[1]*0.8):dim(tra_feat_noNAmice)[1],]\n",
    "test_lab_sam = tra_lab[as.integer(dim(tra_feat_noNAmice)[1]*0.8):dim(tra_feat_noNAmice)[1],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96e016a4-7c84-47c0-85d1-d28b3f5674a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn = -1)\n",
    "# Se comprueba el rendimiento del clasificador en los datos de entrenamiento para comprobar el funcionamiento correcto\n",
    "submission_nb_mice_ova_sub = predict_with_ova(submission_nb_mice_ova_sub, \"nb\", \"ROC\", test_feat_noNAmice_sub, tra_feat_noNAmice_sub, tra_lab_sub, ctrl, grid)\n",
    "options(warn = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b40c166-5dc1-40a6-86ad-f73d6d7fc5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.78164AUC: 0.8260442"
     ]
    }
   ],
   "source": [
    "cat(\"AUC:\", auc(roc(submission_nb_mice_ova_sub$h1n1_vaccine, test_lab_sam$h1n1_vaccine)))\n",
    "cat(\"AUC:\", auc(roc(submission_nb_mice_ova_sub$seasonal_vaccine, test_lab_sam$seasonal_vaccine)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d7ba7-0994-44f1-b31f-8d0342352b0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ova test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbb5d01d-e2e7-4595-a240-f618a06f579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_nb_missF_ova = submission\n",
    "options(warn = -1)\n",
    "# Se ejecuta el ensemble OVA y se guardan las predicciones de las probabilidades en el formato correspondiente para los datos imputados con mice\n",
    "submission_nb_mice_ova = predict_with_ova(submission_nb_missF_ova, \"nb\", \"ROC\", test_feat_noNAmice, tra_feat_noNAmice, tra_lab, ctrl, grid)\n",
    "options(warn = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bb9785f-5310-4bc3-b6d3-93474d39bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda el resultado para subirlo a la plataforma\n",
    "write_csv(submission_nb_mice_ova, \"submission_nb_mice_ova.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "699eb8d4-2283-48b8-8908-94b117072c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_nb_missF_ova = submission\n",
    "options(warn = -1)\n",
    "# Se ejecuta el ensemble OVA y se guardan las predicciones de las probabilidades en el formato correspondiente para los datos imputados con miss forest\n",
    "submission_nb_missF_ova = predict_with_ova(submission_nb_missF_ova, \"nb\", \"ROC\", test_feat_noNAmissF, tra_feat_noNAmissF, tra_lab, ctrl, grid)\n",
    "options(warn = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c89c15f-4c8d-4ff1-9302-e4fc1b8a8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda el resultado para subirlo a la plataforma\n",
    "write_csv(submission_nb_missF_ova, \"submission_nb_missF_ova.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e68d3583-159c-4414-9dea-afe8e94b2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_nb_knn_ova = submission\n",
    "options(warn = -1)\n",
    "# Se ejecuta el ensemble OVA y se guardan las predicciones de las probabilidades en el formato correspondiente para los datos imputados con knn\n",
    "submission_nb_knn_ova = predict_with_ova(submission_nb_knn_ova, \"nb\", \"ROC\", test_feat_noNAknn, tra_feat_noNAknn, tra_lab, ctrl, grid)\n",
    "options(warn = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab6b60cf-9139-4ae9-adcf-eaf639441aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda el resultado para subirlo a la plataforma\n",
    "write_csv(submission_nb_knn_ova, \"submission_nb_knn_ova.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5b4f1-d7fc-4670-a792-14308ca0f517",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ensemble OVO + Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ed99a60-3702-492e-93e3-c7658ce894f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boosting_nb_model = function(train, test, n.weak.learners = 100, probability.calibration = \"friedman\") {\n",
    "  \n",
    "  # Declaramos las variables necesarias para ejecutar el boosting\n",
    "  models.trained = 0\n",
    "  \n",
    "  # Definimos los pesos iniciales para nuestro conjunto de datos de entrenamiento\n",
    "  weigths = rep(c(1/dim(train)[1]), dim(train)[1])\n",
    "    \n",
    "  # Creamos un vector para almacenar las importancias de los clasificadores débiles\n",
    "  weak.learners.importance = c()\n",
    "  \n",
    "  # Creamos una lista en la que almacenar \n",
    "  weak.learners.test.predictions = list()\n",
    "  \n",
    "  # Iteramos mientras queden modelos que entrenar\n",
    "  while(models.trained < n.weak.learners){\n",
    "    \n",
    "    print(paste(\"[Boosting Model] Entrenando clasificador débil número \", models.trained+1, \"...\", sep = \"\"))\n",
    "    \n",
    "    # En el caso de que sea el primer modelo utilizamos como datos el conjunto de entrenamientp\n",
    "    if(models.trained == 0){\n",
    "      sample.indexes = 1:nrow(train)\n",
    "      iteration.data = train\n",
    "    }else{\n",
    "      # En el caso contrario, realizamos una muestra ponderada del conjunto de datos de\n",
    "      # entrenamiento utilizando los pesos calculados\n",
    "      sample.indexes = sample(seq_len(nrow(train)), nrow(train), prob=weigths)\n",
    "      iteration.data = train[sample.indexes, ]\n",
    "    }\n",
    "      \n",
    "    \n",
    "    # Entrenamos el modelo sobre el conjunto de datos de entrenamiento\n",
    "    Fit_seas_i = train(iteration.data[-dim(iteration.data)[1]], make.names(iteration.data$class),\n",
    "              method = \"nb\")\n",
    "\n",
    "    #model.Ripper = JRip(class~., iteration.data)\n",
    "    \n",
    "    # Realizamos predicciones sobre el propio conjunto de datos\n",
    "    #model.Ripper.pred = predict(model.Ripper, newdata = iteration.data)\n",
    "      \n",
    "    model.Ripper.pred = predict(Fit_seas_i, newdata = iteration.data)\n",
    "    \n",
    "    # Calculamos el error del clasificador débil\n",
    "    training.error = 0\n",
    "    for(training.index in 1:length(model.Ripper.pred)){\n",
    "      \n",
    "      # Obtenemos el índice de esta observación al realizar el muestreo\n",
    "      sample.idx = sample.indexes[training.index]\n",
    "        \n",
    "      # Calculamos el error para esta observación de la muestra y la añadimos a nuestros datos\n",
    "      training.error = training.error + weigths[sample.idx]*as.integer(model.Ripper.pred[training.index] == make.names(train[sample.idx, \"class\"]))\n",
    "    }\n",
    "    training.error = training.error / sum(weigths)\n",
    "    \n",
    "    # Calculamos el alpha para actualizar los pesos\n",
    "    alpha = log((1 - training.error) / training.error)\n",
    "    \n",
    "    # Añadimos la importancia de este clasificador a la estructura de datos designada para esta labor\n",
    "    weak.learners.importance = c(weak.learners.importance, alpha)\n",
    "\n",
    "    # Actualizamos los pesos de las observaciones\n",
    "    for(training.index in 1:length(model.Ripper.pred)){\n",
    "      \n",
    "      # Obtenemos el índice de esta observación al realizar el muestreo\n",
    "      sample.idx = sample.indexes[training.index]\n",
    "      \n",
    "      # Calculamos el nuevo peso para esta observación del conjunto de datos\n",
    "      weight.computed = weigths[sample.idx]*exp(alpha*as.integer(model.Ripper.pred[training.index] == make.names(train[sample.idx, \"class\"])))\n",
    "        \n",
    "      # Modificamos el peso en el vector original\n",
    "      weigths[sample.idx] = weight.computed\n",
    "    }\n",
    "      \n",
    "    \n",
    "    \n",
    "    print(paste(\"[Boosting Model] Realizando predicciones con el clasificador débil número \", models.trained+1, \"...\", sep = \"\"))\n",
    "    \n",
    "    # Realizamos con este clasificador débil predicciones sobre el conjunto de datos de evaluación\n",
    "    model.Ripper.test.pred = predict(Fit_seas_i, newdata = iteration.data, type = \"prob\")\n",
    "    \n",
    "    # Añadimos estas predicciones a la lista de resultados\n",
    "    weak.learners.test.predictions[[models.trained + 1]] = model.Ripper.test.pred\n",
    "    \n",
    "    print(paste(\"[Boosting Model] Entrenamiento y evaluación del clasificador débil número \", models.trained+1, \" completado.\", sep = \"\"))\n",
    "  \n",
    "    # Añadimos una unidad a los modelos ya entrenados\n",
    "    models.trained = models.trained + 1\n",
    "  }\n",
    "  \n",
    "  print(\"[Boosting Model] Agregando la salida de los modelos...\")\n",
    "  \n",
    "    \n",
    "  # Calculamos la salida del modelo para cada observación del conjunto de datos de evaluación\n",
    "  final.output.boosting = c()\n",
    "  for(test.observation.index in 1:(nrow(test)/2)) {\n",
    "    \n",
    "    # Para cada uno de los clasificadores, multiplicamos su salida por la importancia del clasificador\n",
    "    # output.boosting = sapply(1:n.weak.learners, function(weak.model.index) \n",
    "    #  weak.learners.importance[weak.model.index] * ifelse(weak.learners.test.predictions[[weak.model.index]][test.observation.index] == 1, 1, -1))\n",
    "    # Añadimos el output final de este modelo a un vector\n",
    "    #final.output.boosting = c(final.output.boosting, sum(unlist(output.boosting)))\n",
    "\n",
    "    # Obtenemos las predicciones de los diferentes clasificadores para esta observación\n",
    "    predictions.for.observation = unlist(lapply(weak.learners.test.predictions, function(predictions) predictions[test.observation.index,]))\n",
    "    # Calculamos la probabilidad haciendo uso del paper de Friedman\n",
    "    fx = 0.5 * log((sum(predictions.for.observation[2]>predictions.for.observation[1]) + sum(predictions.for.observation[4]>predictions.for.observation[3]))/length(predictions.for.observation)\n",
    "                   / (sum(predictions.for.observation[1]>predictions.for.observation[2]) + (sum(predictions.for.observation[3]>predictions.for.observation[4])))/length(predictions.for.observation))\n",
    "    boosting.prob = 1 / (1 + exp(-2*fx))\n",
    "    \n",
    "    # Añadimos la predicción de probabilidad para esta observación en nuestra estructura de datos\n",
    "    final.output.boosting = c(final.output.boosting, boosting.prob)\n",
    "                                                \n",
    "    # Calculamos la probabilidad haciendo uso del paper de Friedman\n",
    "    fx = 0.5 * log((sum(predictions.for.observation[4]>predictions.for.observation[3]))/length(predictions.for.observation)\n",
    "                   / ((sum(predictions.for.observation[3]>predictions.for.observation[4])))/length(predictions.for.observation))\n",
    "    boosting.prob = 1 / (1 + exp(-2*fx))\n",
    "    \n",
    "    # Añadimos la predicción de probabilidad para esta observación en nuestra estructura de datos\n",
    "    final.output.boosting = c(final.output.boosting, boosting.prob)\n",
    "  }\n",
    "  \n",
    "  print(\"[Boosting Model] Modelo de Boosting instanciado y evaluado con éxito.\")\n",
    "                                     \n",
    "  final.output.boosting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e6f6c3-2ea0-4baa-82cb-6ffada8ebecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tra_feat_noNAknn\n",
    "X_test = test_feat_noNAknn\n",
    "\n",
    "options(warn = -1)\n",
    "\n",
    "# Cargamos el formato de submission y las etiquetas correspondientes a las clases del conjunto de entrenamiento\n",
    "y_train = read.csv(\"training_set_labels.csv\", header=TRUE, sep=',', na.strings = c('?','', 'NA'))\n",
    "submission_format = read.csv(\"submission_format.csv\", header=TRUE, sep=',', na.strings = c('?','', 'NA'))\n",
    "\n",
    "# Modificamos las clases para crear una única variable multiclase y la añadimos a nuestros datos de entrenamiento\n",
    "X_train['class'] = strtoi(paste(y_train$h1n1_vaccine, y_train$seasonal_vaccine, sep=\"\"), base = 2)\n",
    "\n",
    "# Convertimos esta clase en un factor\n",
    "X_train = X_train %>%\n",
    "  mutate(class = factor(class, levels = c(0, 1, 2, 3), labels = c(\"Unvaccinated\", \"Vaccinated (Seasonal)\", \"Vaccinated (H1N1)\", \"Vaccinated (Both)\")))\n",
    "\n",
    "# Obtenemos todas las parejas posibles de clases que existen\n",
    "class_pairs = list()\n",
    "class_pairs[[1]] = c(levels(X_train$class)[1], levels(X_train$class)[2])\n",
    "class_pairs[[2]] = c(levels(X_train$class)[1], levels(X_train$class)[3])\n",
    "class_pairs[[3]] = c(levels(X_train$class)[1], levels(X_train$class)[4])\n",
    "class_pairs[[4]] = c(levels(X_train$class)[2], levels(X_train$class)[3])\n",
    "class_pairs[[5]] = c(levels(X_train$class)[2], levels(X_train$class)[4])\n",
    "class_pairs[[6]] = c(levels(X_train$class)[3], levels(X_train$class)[4])\n",
    "\n",
    "# Creamos una función que dado lo siguiente:\n",
    "#  - Un conjunto de datos de entrenamiento con una columna 'class' con las clases a predecir\n",
    "#  - El conjunto de datos de evaluación a predecir\n",
    "#  - Un vector de etiquetas con las que filtrar los datos para entrenar el modelo\n",
    "# Devuelve un vector con la probabilidad de la primera clase a predecir\n",
    "calculate_probability_ovo_boosting_model = function(train_data, test_data, filtered_classes, n.weak.learners = 256) {\n",
    "  \n",
    "  # Filtramos el conjunto de datos por las librerias especificadas\n",
    "  X_train_filtered = train_data %>%\n",
    "    filter(class %in% filtered_classes)\n",
    "  \n",
    "  # Redefinimos el factor para no generar problemas con Caret\n",
    "  X_train_filtered = X_train_filtered %>%\n",
    "    mutate(class = factor(class, levels = filtered_classes))\n",
    "  \n",
    "  # Transformamos la clase a 0 y 1\n",
    "  X_train_filtered = X_train_filtered %>%\n",
    "    mutate(class = ifelse(class == filtered_classes[1], 1, 0))\n",
    "  \n",
    "  # Lo volvemos a convertir en un factor\n",
    "  X_train_filtered = X_train_filtered %>%\n",
    "    mutate(class = as.factor(class))\n",
    "  \n",
    "  # Creamos el modelo de boosting y evaluamos sobre el conjunto de evaluación\n",
    "  boosting.result = boosting_nb_model(X_train_filtered, test_data, n.weak.learners = n.weak.learners)\n",
    "  boosting.result\n",
    "}\n",
    "\n",
    "# Lanzamos este método para cada una de las parejas de clases que hemos planteado\n",
    "predicted.ovo = lapply(class_pairs, function(pair) calculate_probability_ovo_boosting_model(X_train, X_test, unlist(pair), n.weak.learners = 2))\n",
    "predicted.ovo\n",
    "\n",
    "# Definimos el número de clases\n",
    "K = 4\n",
    "\n",
    "# Declaramos un vector de etiquetas con los nombres de las probabilidades obtenidas\n",
    "probabilities_names = unlist(lapply(class_pairs, function(pair) paste(pair[1], pair[2], sep=\" Vs. \")))\n",
    "\n",
    "# Creamos dos vectores para almacenar los resultados para cada una de las vacunas\n",
    "h1n1.probabilities = c()\n",
    "seas.probabilities = c()\n",
    "\n",
    "# Iteramos sobre cada uno de los elementos de las predicciones devueltas por los modelos OVO\n",
    "for(prediction.index in 1:length(predicted.ovo[[1]])){\n",
    "  \n",
    "  # Obtenemos un array con las predicciones para la instancia en concreto\n",
    "  probabilities_vector = unlist(lapply(predicted.ovo, function(predictions) predictions[prediction.index]))\n",
    "  probabilities_array = array(probabilities_vector, dim=c(1,(K*(K-1)/2)))\n",
    "  colnames(probabilities_array) = probabilities_names\n",
    "  \n",
    "  # Obtenemos la matriz con las probabilidades\n",
    "  Q = matrix(0,K,K)\n",
    "  Q[lower.tri(Q)] = 1 - probabilities_array\n",
    "  Qt = t(Q)\n",
    "  Q[upper.tri(Q)] = 1 - Qt[upper.tri(Qt)]\n",
    "  diag(Q) = rowSums(Q)\n",
    "  Q = Q / (K-1)\n",
    "  \n",
    "  # Creamos un vector de probabilidades para cada clase inicial que sume 1\n",
    "  p = rbeta(K,1,1)\n",
    "  p = p/sum(p)\n",
    "  \n",
    "  # Actualizamos el vector de probabilidades hasta que el equilibrio ha sido alcanzado utilizando\n",
    "  # como referencia: Probability Estimates for Multi-class Classification by Pairwise Coupling, Wu et al. (2003)\n",
    "  for(i in 1:1000) p = Q%*%p\n",
    "                                       \n",
    "  \n",
    "  # Formateamos un poco el vector de probabilidades resultante\n",
    "  final.probability = as.vector(t(p))\n",
    "  names(final.probability) = levels(X_train$class)\n",
    "  \n",
    "  # Calculamos la probabilidad de que haya tomado ambas vacunas siguiendo el formato establecido\n",
    "  h1n1.vaccine.prob = final.probability['Vaccinated (H1N1)'] + final.probability['Vaccinated (Both)']\n",
    "  seas.vaccine.prob = final.probability['Vaccinated (Seasonal)'] + final.probability['Vaccinated (Both)']\n",
    "  \n",
    "  # Añadimos estas probabilidades a los vectores en los que las almacenamos\n",
    "  h1n1.probabilities = append(h1n1.probabilities, h1n1.vaccine.prob)\n",
    "  seas.probabilities = append(seas.probabilities, seas.vaccine.prob)\n",
    "}\n",
    "\n",
    "# Añadimos al submission format las predicciones de probabilidad \n",
    "submission_format['h1n1_vaccine'] = h1n1.probabilities\n",
    "submission_format['seasonal_vaccine'] = seas.probabilities\n",
    "submission_format\n",
    "\n",
    "options(warn = -1)\n",
    "                                       \n",
    "# Guardamos en disco los resultados\n",
    "write.csv(submission_format,\"submission_nb_knn_ovo_boosting.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c53a1771-cc9b-4653-9693-5faaf323725f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>respondent_id</th><th scope=col>h1n1_vaccine</th><th scope=col>seasonal_vaccine</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>11348</th><td>38054</td><td>0</td><td>7.188294e-177</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & respondent\\_id & h1n1\\_vaccine & seasonal\\_vaccine\\\\\n",
       "  & <int> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t11348 & 38054 & 0 & 7.188294e-177\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 3\n",
       "\n",
       "| <!--/--> | respondent_id &lt;int&gt; | h1n1_vaccine &lt;dbl&gt; | seasonal_vaccine &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 11348 | 38054 | 0 | 7.188294e-177 |\n",
       "\n"
      ],
      "text/plain": [
       "      respondent_id h1n1_vaccine seasonal_vaccine\n",
       "11348 38054         0            7.188294e-177   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_format[11348,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1325d1e-86e8-420b-9695-6c8a3534a3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>respondent_id</th><th scope=col>h1n1_vaccine</th><th scope=col>seasonal_vaccine</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>11349</th><td>38055</td><td>NA</td><td>NA</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & respondent\\_id & h1n1\\_vaccine & seasonal\\_vaccine\\\\\n",
       "  & <int> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t11349 & 38055 & NA & NA\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 3\n",
       "\n",
       "| <!--/--> | respondent_id &lt;int&gt; | h1n1_vaccine &lt;dbl&gt; | seasonal_vaccine &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 11349 | 38055 | NA | NA |\n",
       "\n"
      ],
      "text/plain": [
       "      respondent_id h1n1_vaccine seasonal_vaccine\n",
       "11349 38055         NA           NA              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_format[11349,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66256aa7-bc2e-4acb-a5bd-63dfd4b71441",
   "metadata": {},
   "source": [
    "A partir del 11349 son todo NAs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
